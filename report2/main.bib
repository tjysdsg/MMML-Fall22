@article{qa_overlap,
    author = {Patrick S. H. Lewis and
 Pontus Stenetorp and
 Sebastian Riedel},
    title = {Question and Answer Test-Train Overlap in Open-Domain Question Answering
 Datasets},
    journal = {CoRR},
    volume = {abs/2008.02637},
    year = {2020},
    url = {https://arxiv.org/abs/2008.02637},
    eprinttype = {arXiv},
    eprint = {2008.02637},
    timestamp = {Thu, 14 Oct 2021 09:14:40 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2008-02637.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/GoyalKSBP16,
    author = {Yash Goyal and
 Tejas Khot and
 Douglas Summers{-}Stay and
 Dhruv Batra and
 Devi Parikh},
    title = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding
 in Visual Question Answering},
    journal = {CoRR},
    volume = {abs/1612.00837},
    year = {2016},
    url = {http://arxiv.org/abs/1612.00837},
    eprinttype = {arXiv},
    eprint = {1612.00837},
    timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
    biburl = {https://dblp.org/rec/journals/corr/GoyalKSBP16.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{singh-etal-2021-mimoqa,
    title = "{MIMOQA}: Multimodal Input Multimodal Output Question Answering",
    author = "Singh, Hrituraj  and
      Nasery, Anshul  and
      Mehta, Denil  and
      Agarwal, Aishwarya  and
      Lamba, Jatin  and
      Srinivasan, Balaji Vasan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.418",
    doi = "10.18653/v1/2021.naacl-main.418",
    pages = "5317--5332",
    abstract = "Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task - MIMOQA - Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.",
}

@inproceedings{chen-etal-2020-hybridqa,
    title = "{H}ybrid{QA}: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data",
    author = "Chen, Wenhu  and
      Zha, Hanwen  and
      Chen, Zhiyu  and
      Xiong, Wenhan  and
      Wang, Hong  and
      Wang, William Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.91",
    doi = "10.18653/v1/2020.findings-emnlp.91",
    pages = "1026--1036",
    abstract = "Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20{\%}, while the hybrid model can achieve an EM over 40{\%}. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model{'}s score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information.",
}

@article{DBLP:journals/corr/abs-2104-06039,
    author = {Alon Talmor and
 Ori Yoran and
 Amnon Catav and
 Dan Lahav and
 Yizhong Wang and
 Akari Asai and
 Gabriel Ilharco and
 Hannaneh Hajishirzi and
 Jonathan Berant},
    title = {MultiModalQA: Complex Question Answering over Text, Tables and Images},
    journal = {CoRR},
    volume = {abs/2104.06039},
    year = {2021},
    url = {https://arxiv.org/abs/2104.06039},
    eprinttype = {arXiv},
    eprint = {2104.06039},
    timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2104-06039.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2001-08034,
    author = {Darryl Hannan and
 Akshay Jain and
 Mohit Bansal},
    title = {ManyModalQA: Modality Disambiguation and {QA} over Diverse Inputs},
    journal = {CoRR},
    volume = {abs/2001.08034},
    year = {2020},
    url = {https://arxiv.org/abs/2001.08034},
    eprinttype = {arXiv},
    eprint = {2001.08034},
    timestamp = {Fri, 24 Jan 2020 15:00:57 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-2001-08034.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2106.02280,
    doi = {10.48550/ARXIV.2106.02280},

    url = {https://arxiv.org/abs/2106.02280},

    author = {Sheng, Sasha and Singh, Amanpreet and Goswami, Vedanuj and Magana, Jose Alberto Lopez and Galuba, Wojciech and Parikh, Devi and Kiela, Douwe},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Human-Adversarial Visual Question Answering},

    publisher = {arXiv},

    year = {2021},

    copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2106.00245,
    doi = {10.48550/ARXIV.2106.00245},

    url = {https://arxiv.org/abs/2106.00245},

    author = {Li, Linjie and Lei, Jie and Gan, Zhe and Liu, Jingjing},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models},

    publisher = {arXiv},

    year = {2021},

    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{DBLP:journals/corr/abs-1908-08530,
    author = {Weijie Su and
 Xizhou Zhu and
 Yue Cao and
 Bin Li and
 Lewei Lu and
 Furu Wei and
 Jifeng Dai},
    title = {{VL-BERT:} Pre-training of Generic Visual-Linguistic Representations},
    journal = {CoRR},
    volume = {abs/1908.08530},
    year = {2019},
    url = {http://arxiv.org/abs/1908.08530},
    eprinttype = {arXiv},
    eprint = {1908.08530},
    timestamp = {Tue, 12 Apr 2022 21:46:11 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1908-08530.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{47761,
    title	 = {Natural Questions: a Benchmark for Question Answering Research},
    author	 = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
    year	 = {2019},
    journal	 = {Transactions of the Association of Computational Linguistics}
}


@article{DBLP:journals/corr/NguyenRSGTMD16,
    author = {Tri Nguyen and
 Mir Rosenberg and
 Xia Song and
 Jianfeng Gao and
 Saurabh Tiwary and
 Rangan Majumder and
 Li Deng},
    title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
    journal = {CoRR},
    volume = {abs/1611.09268},
    year = {2016},
    url = {http://arxiv.org/abs/1611.09268},
    eprinttype = {arXiv},
    eprint = {1611.09268},
    timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},
    biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{webqa,
    author = {Yingshan Chang and
 Mridu Narang and
 Hisami Suzuki and
 Guihong Cao and
 Jianfeng Gao and
 Yonatan Bisk},
    title = {WebQA: Multihop and Multimodal {QA}},
    journal = {CoRR},
    volume = {abs/2109.00590},
    year = {2021},
    url = {https://arxiv.org/abs/2109.00590},
    eprinttype = {arXiv},
    eprint = {2109.00590},
    timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2109-00590.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{bert,
    author = {Jacob Devlin and
 Ming{-}Wei Chang and
 Kenton Lee and
 Kristina Toutanova},
    title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
 Understanding},
    journal = {CoRR},
    volume = {abs/1810.04805},
    year = {2018},
    url = {http://arxiv.org/abs/1810.04805},
    eprinttype = {arXiv},
    eprint = {1810.04805},
    timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2208.02532,
    doi = {10.48550/ARXIV.2208.02532},

    url = {https://arxiv.org/abs/2208.02532},

    author = {Yang, Hao and Lin, Junyang and Yang, An and Wang, Peng and Zhou, Chang and Yang, Hongxia},

    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Prompt Tuning for Generative Multimodal Pretrained Models},

    publisher = {arXiv},

    year = {2022},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{DBLP:journals/corr/abs-2106-01561,
    author = {Cunxiang Wang and
 Pai Liu and
 Yue Zhang},
    title = {Can Generative Pre-trained Language Models Serve as Knowledge Bases
 for Closed-book QA?},
    journal = {CoRR},
    volume = {abs/2106.01561},
    year = {2021},
    url = {https://arxiv.org/abs/2106.01561},
    eprinttype = {arXiv},
    eprint = {2106.01561},
    timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2106-01561.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vlp,
    author = {Luowei Zhou and
 Hamid Palangi and
 Lei Zhang and
 Houdong Hu and
 Jason J. Corso and
 Jianfeng Gao},
    title = {Unified Vision-Language Pre-Training for Image Captioning and {VQA}},
    journal = {CoRR},
    volume = {abs/1909.11059},
    year = {2019},
    url = {http://arxiv.org/abs/1909.11059},
    eprinttype = {arXiv},
    eprint = {1909.11059},
    timestamp = {Fri, 21 May 2021 15:45:48 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1909-11059.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{VinVL,
    author = {Pengchuan Zhang and
 Xiujun Li and
 Xiaowei Hu and
 Jianwei Yang and
 Lei Zhang and
 Lijuan Wang and
 Yejin Choi and
 Jianfeng Gao},
    title = {VinVL: Making Visual Representations Matter in Vision-Language Models},
    journal = {CoRR},
    volume = {abs/2101.00529},
    year = {2021},
    url = {https://arxiv.org/abs/2101.00529},
    eprinttype = {arXiv},
    eprint = {2101.00529},
    timestamp = {Tue, 15 Feb 2022 08:52:23 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-2101-00529.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{okvqa,
    doi = {10.48550/ARXIV.1906.00067},

    url = {https://arxiv.org/abs/1906.00067},

    author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge},

    publisher = {arXiv},

    year = {2019},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{kbvqa,
    doi = {10.48550/ARXIV.1511.02570},

    url = {https://arxiv.org/abs/1511.02570},

    author = {Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Explicit Knowledge-based Reasoning for Visual Question Answering},

    publisher = {arXiv},

    year = {2015},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{PICa,
    doi = {10.48550/ARXIV.2109.05014},

    url = {https://arxiv.org/abs/2109.05014},

    author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA},

    publisher = {arXiv},

    year = {2021},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{GPT3,
    doi = {10.48550/ARXIV.2005.14165},

    url = {https://arxiv.org/abs/2005.14165},

    author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},

    keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Language Models are Few-Shot Learners},

    publisher = {arXiv},

    year = {2020},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{image_paragraph,
    doi = {10.48550/ARXIV.1611.06607},

    url = {https://arxiv.org/abs/1611.06607},

    author = {Krause, Jonathan and Johnson, Justin and Krishna, Ranjay and Fei-Fei, Li},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {A Hierarchical Approach for Generating Descriptive Image Paragraphs},

    publisher = {arXiv},

    year = {2016},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Flamingo,
    doi = {10.48550/ARXIV.2204.14198},

    url = {https://arxiv.org/abs/2204.14198},

    author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Flamingo: a Visual Language Model for Few-Shot Learning},

    publisher = {arXiv},

    year = {2022},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{VLMO,
    author = {Wenhui Wang and
 Hangbo Bao and
 Li Dong and
 Furu Wei},
    title = {VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts},
    journal = {CoRR},
    volume = {abs/2111.02358},
    year = {2021},
    url = {https://arxiv.org/abs/2111.02358},
    eprinttype = {arXiv},
    eprint = {2111.02358},
    timestamp = {Fri, 05 Nov 2021 15:25:54 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-2111-02358.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{frozen,
    doi = {10.48550/ARXIV.2106.13884},

    url = {https://arxiv.org/abs/2106.13884},

    author = {Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, S. M. Ali and Vinyals, Oriol and Hill, Felix},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Multimodal Few-Shot Learning with Frozen Language Models},

    publisher = {arXiv},

    year = {2021},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{nlvr2,
    author = {Alane Suhr and
 Stephanie Zhou and
 Iris Zhang and
 Huajun Bai and
 Yoav Artzi},
    title = {A Corpus for Reasoning About Natural Language Grounded in Photographs},
    journal = {CoRR},
    volume = {abs/1811.00491},
    year = {2018},
    url = {http://arxiv.org/abs/1811.00491},
    eprinttype = {arXiv},
    eprint = {1811.00491},
    timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-1811-00491.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{antol2015vqa,
    title = {Vqa: Visual question answering},
    author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
    booktitle = {Proceedings of the IEEE international conference on computer vision},
    pages = {2425--2433},
    year = {2015}
}

@article{yuan2021bartscore,
    title = {Bartscore: Evaluating generated text as text generation},
    author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
    journal = {Advances in Neural Information Processing Systems},
    volume = {34},
    pages = {27263--27277},
    year = {2021}
}

@article{bm25,
    title = {The probabilistic relevance framework: BM25 and beyond},
    author = {Robertson, Stephen and Zaragoza, Hugo and others},
    journal = {Foundations and Trends{\textregistered} in Information Retrieval},
    volume = {3},
    number = {4},
    pages = {333--389},
    year = {2009},
    publisher = {Now Publishers, Inc.}
}

@inproceedings{clip,
    title = {Learning transferable visual models from natural language supervision},
    author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
    booktitle = {International Conference on Machine Learning},
    pages = {8748--8763},
    year = {2021},
    organization = {PMLR}
}


@misc{visual7w,
    doi = {10.48550/ARXIV.1511.03416},

    url = {https://arxiv.org/abs/1511.03416},

    author = {Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Visual7W: Grounded Question Answering in Images},

    publisher = {arXiv},

    year = {2015},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.1809.01124,
    doi = {10.48550/ARXIV.1809.01124},

    url = {https://arxiv.org/abs/1809.01124},

    author = {Narasimhan, Medhini and Schwing, Alexander G.},

    keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},

    title = {Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering},

    publisher = {arXiv},

    year = {2018},

    copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{transformer,
    title = {Attention is all you need},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal = {Advances in neural information processing systems},
    volume = {30},
    year = {2017}
}

@article{lxmert,
    title = {Lxmert: Learning cross-modality encoder representations from transformers},
    author = {Tan, Hao and Bansal, Mohit},
    journal = {arXiv preprint arXiv:1908.07490},
    year = {2019}
}