@misc{closed-domain,
  doi = {10.48550/ARXIV.1511.03416},
  
  url = {https://arxiv.org/abs/1511.03416},
  
  author = {Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Visual7W: Grounded Question Answering in Images},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{webqa,
  doi = {10.48550/ARXIV.2109.00590},
  
  url = {https://arxiv.org/abs/2109.00590},
  
  author = {Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {WebQA: Multihop and Multimodal QA},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{vlp,
  doi = {10.48550/ARXIV.1909.11059},
  
  url = {https://arxiv.org/abs/1909.11059},
  
  author = {Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason J. and Gao, Jianfeng},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unified Vision-Language Pre-Training for Image Captioning and VQA},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{vlp-survey,
  doi = {10.48550/ARXIV.2202.09061},
  
  url = {https://arxiv.org/abs/2202.09061},
  
  author = {Chen, Feilong and Zhang, Duzhen and Han, Minglun and Chen, Xiuyi and Shi, Jing and Xu, Shuang and Xu, Bo},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {VLP: A Survey on Vision-Language Pre-training},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@misc{okvqa,
  doi = {10.48550/ARXIV.1906.00067},
  
  url = {https://arxiv.org/abs/1906.00067},
  
  author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{mimoqa,
    title = "{MIMOQA}: Multimodal Input Multimodal Output Question Answering",
    author = "Singh, Hrituraj  and
      Nasery, Anshul  and
      Mehta, Denil  and
      Agarwal, Aishwarya  and
      Lamba, Jatin  and
      Srinivasan, Balaji Vasan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.418",
    doi = "10.18653/v1/2021.naacl-main.418",
    pages = "5317--5332",
    abstract = "Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task - MIMOQA - Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics.",
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  eprinttype = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vit,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  journal   = {CoRR},
  volume    = {abs/2010.11929},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint    = {2010.11929},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{PICa,
  doi = {10.48550/ARXIV.2109.05014},
  
  url = {https://arxiv.org/abs/2109.05014},
  
  author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{ITM_cross_attn,
author = {Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
title = {Stacked Cross Attention for Image-Text Matching},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@article{albef,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}

@article{BERT,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vilbert,
  author    = {Jiasen Lu and
               Dhruv Batra and
               Devi Parikh and
               Stefan Lee},
  title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations
               for Vision-and-Language Tasks},
  journal   = {CoRR},
  volume    = {abs/1908.02265},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.02265},
  eprinttype = {arXiv},
  eprint    = {1908.02265},
  timestamp = {Fri, 09 Aug 2019 12:15:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-02265.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{itm-feifei,
  doi = {10.48550/ARXIV.1412.2306},
  
  url = {https://arxiv.org/abs/1412.2306},
  
  author = {Karpathy, Andrej and Fei-Fei, Li},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{rcnn,
  author    = {Shaoqing Ren and
               Kaiming He and
               Ross B. Girshick and
               Jian Sun},
  title     = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
               Networks},
  journal   = {CoRR},
  volume    = {abs/1506.01497},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.01497},
  eprinttype = {arXiv},
  eprint    = {1506.01497},
  timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RenHG015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vilt,
  title={Vilt: Vision-and-language transformer without convolution or region supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={International Conference on Machine Learning},
  pages={5583--5594},
  year={2021},
  organization={PMLR}
}

@article{visualgenome,
    author = {Ranjay Krishna and
 Yuke Zhu and
 Oliver Groth and
 Justin Johnson and
 Kenji Hata and
 Joshua Kravitz and
 Stephanie Chen and
 Yannis Kalantidis and
 Li{-}Jia Li and
 David A. Shamma and
 Michael S. Bernstein and
 Li Fei{-}Fei},
    title = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense
 Image Annotations},
    journal = {Int. J. Comput. Vis.},
    volume = {123},
    number = {1},
    pages = {32--73},
    year = {2017},
    url = {https://doi.org/10.1007/s11263-016-0981-7},
    doi = {10.1007/s11263-016-0981-7},
    timestamp = {Sun, 02 Oct 2022 15:37:22 +0200},
    biburl = {https://dblp.org/rec/journals/ijcv/KrishnaZGJHKCKL17.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bartscore,
  author    = {Weizhe Yuan and
               Graham Neubig and
               Pengfei Liu},
  title     = {BARTScore: Evaluating Generated Text as Text Generation},
  journal   = {CoRR},
  volume    = {abs/2106.11520},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.11520},
  eprinttype = {arXiv},
  eprint    = {2106.11520},
  timestamp = {Wed, 30 Jun 2021 16:14:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-11520.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{itm-semantic,
  doi = {10.48550/ARXIV.1909.02701},
  
  url = {https://arxiv.org/abs/1909.02701},
  
  author = {Li, Kunpeng and Zhang, Yulun and Li, Kai and Li, Yuanyuan and Fu, Yun},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Visual Semantic Reasoning for Image-Text Matching},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{itm-quintuplet,
  doi = {10.48550/ARXIV.2003.03669},
  
  url = {https://arxiv.org/abs/2003.03669},
  
  author = {Chen, Tianlang and Deng, Jiajun and Luo, Jiebo},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adaptive Offline Quintuplet Loss for Image-Text Matching},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{itm-bert,
  doi = {10.48550/ARXIV.2005.09801},
  
  url = {https://arxiv.org/abs/2005.09801},
  
  author = {Gao, Dehong and Jin, Linbo and Chen, Ben and Qiu, Minghui and Li, Peng and Wei, Yi and Hu, Yi and Wang, Hao},
  
  keywords = {Information Retrieval (cs.IR), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{mult,
  doi = {10.48550/ARXIV.1906.00295},
  
  url = {https://arxiv.org/abs/1906.00295},
  
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J. Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Transformer for Unaligned Multimodal Language Sequences},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{unimodal-distill,
  doi = {10.48550/ARXIV.2204.10496},
  
  url = {https://arxiv.org/abs/2204.10496},
  
  author = {Wang, Zhecan and Codella, Noel and Chen, Yen-Chun and Zhou, Luowei and Dai, Xiyang and Xiao, Bin and Yang, Jianwei and You, Haoxuan and Chang, Kai-Wei and Chang, Shih-fu and Yuan, Lu},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{distill-text,
  doi = {10.48550/ARXIV.2012.04584},
  
  url = {https://arxiv.org/abs/2012.04584},
  
  author = {Izacard, Gautier and Grave, Edouard},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distilling Knowledge from Reader to Retriever for Question Answering},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{distill-vl,
  doi = {10.48550/ARXIV.2104.02096},
  
  url = {https://arxiv.org/abs/2104.02096},
  
  author = {Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Compressing Visual-linguistic Model via Knowledge Distillation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{minivlm,
  doi = {10.48550/ARXIV.2012.06946},
  
  url = {https://arxiv.org/abs/2012.06946},
  
  author = {Wang, Jianfeng and Hu, Xiaowei and Zhang, Pengchuan and Li, Xiujun and Wang, Lijuan and Zhang, Lei and Gao, Jianfeng and Liu, Zicheng},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MiniVLM: A Smaller and Faster Vision-Language Model},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{efficient-net,
  doi = {10.48550/ARXIV.1905.11946},
  
  url = {https://arxiv.org/abs/1905.11946},
  
  author = {Tan, Mingxing and Le, Quoc V.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{BiFPN,
  doi = {10.48550/ARXIV.1911.09070},
  
  url = {https://arxiv.org/abs/1911.09070},
  
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {EfficientDet: Scalable and Efficient Object Detection},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{minilm,
  doi = {10.48550/ARXIV.2002.10957},
  
  url = {https://arxiv.org/abs/2002.10957},
  
  author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{kd-seq,
  doi = {10.48550/ARXIV.1606.07947},
  
  url = {https://arxiv.org/abs/1606.07947},
  
  author = {Kim, Yoon and Rush, Alexander M.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Sequence-Level Knowledge Distillation},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{distillbert,
  doi = {10.48550/ARXIV.1910.01108},
  
  url = {https://arxiv.org/abs/1910.01108},
  
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{adapt-distill,
  doi = {10.48550/ARXIV.2204.10496},
  
  url = {https://arxiv.org/abs/2204.10496},
  
  author = {Wang, Zhecan and Codella, Noel and Chen, Yen-Chun and Zhou, Luowei and Dai, Xiyang and Xiao, Bin and Yang, Jianwei and You, Haoxuan and Chang, Kai-Wei and Chang, Shih-fu and Yuan, Lu},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@InProceedings{itm-cross,
author = {Wei, Xi and Zhang, Tianzhu and Li, Yan and Zhang, Yongdong and Wu, Feng},
title = {Multi-Modality Cross Attention Network for Image and Sentence Matching},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}


